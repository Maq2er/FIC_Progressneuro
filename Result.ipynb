{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10066219,"sourceType":"datasetVersion","datasetId":6201255}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Блок 1: Установка необходимых библиотек\n\n# Устанавливаем библиотеки для работы с моделями, OCR, обработкой текста и прочее\n# Установка зависимостей\n\n# Устанавливаем совместимую версию protobuf для корректной работы с sentencepiece\n!pip install tensorflow==2.11.0\n!pip install protobuf==3.20.1  # Устанавливаем совместимую версию protobuf\n!pip install faiss-cpu==1.9.0.post1 --no-deps\n!pip install transformers easyocr spacy datasets\n!pip install bitsandbytes --upgrade  # Устанавливаем bitsandbytes для квантования в 4 бита\n!pip install sentencepiece  # Устанавливаем sentencepiece для корректной работы с моделями\n\n# Устанавливаем дополнительные библиотеки\n!pip install pandas scikit-learn sentence_transformers python-docx openpyxl accelerate\n!python -m spacy download ru_core_news_sm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T17:37:43.127514Z","iopub.execute_input":"2024-12-01T17:37:43.127817Z","iopub.status.idle":"2024-12-01T17:40:07.766988Z","shell.execute_reply.started":"2024-12-01T17:37:43.127789Z","shell.execute_reply":"2024-12-01T17:40:07.765848Z"}},"outputs":[{"name":"stdout","text":"Collecting tensorflow==2.11.0\n  Downloading tensorflow-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.1 kB)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (1.6.3)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (24.3.25)\nCollecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.11.0)\n  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (1.62.2)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (3.11.0)\nCollecting keras<2.12,>=2.11.0 (from tensorflow==2.11.0)\n  Downloading keras-2.11.0-py2.py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (18.1.1)\nRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (21.3)\nCollecting protobuf<3.20,>=3.9.2 (from tensorflow==2.11.0)\n  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (70.0.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (1.16.0)\nCollecting tensorboard<2.12,>=2.11 (from tensorflow==2.11.0)\n  Downloading tensorboard-2.11.2-py3-none-any.whl.metadata (1.9 kB)\nCollecting tensorflow-estimator<2.12,>=2.11.0 (from tensorflow==2.11.0)\n  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (4.12.2)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (1.16.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.11.0) (0.37.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.11.0) (0.43.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.30.0)\nCollecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.12,>=2.11->tensorflow==2.11.0)\n  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.6)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.32.3)\nCollecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.12,>=2.11->tensorflow==2.11.0)\n  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\nCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.12,>=2.11->tensorflow==2.11.0)\n  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.0.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow==2.11.0) (3.1.2)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.0.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.1.5)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.6.0)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.2.2)\nDownloading tensorflow-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\nDownloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\nDownloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: tensorboard-plugin-wit, tensorflow-estimator, tensorboard-data-server, protobuf, keras, gast, google-auth-oauthlib, tensorboard, tensorflow\n  Attempting uninstall: tensorflow-estimator\n    Found existing installation: tensorflow-estimator 2.15.0\n    Uninstalling tensorflow-estimator-2.15.0:\n      Successfully uninstalled tensorflow-estimator-2.15.0\n  Attempting uninstall: tensorboard-data-server\n    Found existing installation: tensorboard-data-server 0.7.2\n    Uninstalling tensorboard-data-server-0.7.2:\n      Successfully uninstalled tensorboard-data-server-0.7.2\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n  Attempting uninstall: keras\n    Found existing installation: keras 3.3.3\n    Uninstalling keras-3.3.3:\n      Successfully uninstalled keras-3.3.3\n  Attempting uninstall: gast\n    Found existing installation: gast 0.5.4\n    Uninstalling gast-0.5.4:\n      Successfully uninstalled gast-0.5.4\n  Attempting uninstall: google-auth-oauthlib\n    Found existing installation: google-auth-oauthlib 1.2.0\n    Uninstalling google-auth-oauthlib-1.2.0:\n      Successfully uninstalled google-auth-oauthlib-1.2.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.16.2\n    Uninstalling tensorboard-2.16.2:\n      Successfully uninstalled tensorboard-2.16.2\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.16.1\n    Uninstalling tensorflow-2.16.1:\n      Successfully uninstalled tensorflow-2.16.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.0.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\ngoogle-ai-generativelanguage 0.6.10 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-datastore 2.20.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\ngoogle-cloud-language 2.14.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\ngoogle-cloud-spanner 3.47.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\ngoogle-cloud-videointelligence 2.13.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nonnx 1.17.0 requires protobuf>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\ntensorboardx 2.6.2.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\ntensorflow-datasets 4.9.6 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\ntensorflow-decision-forests 1.9.1 requires tensorflow~=2.16.1, but you have tensorflow 2.11.0 which is incompatible.\ntensorflow-serving-api 2.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\ntensorflow-serving-api 2.16.1 requires tensorflow<3,>=2.16.1, but you have tensorflow 2.11.0 which is incompatible.\ntensorflow-text 2.16.1 requires tensorflow<2.17,>=2.16.1; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.11.0 which is incompatible.\ntf-keras 2.16.0 requires tensorflow<2.17,>=2.16, but you have tensorflow 2.11.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-0.4.6 keras-2.11.0 protobuf-3.19.6 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0\nCollecting protobuf==3.20.1\n  Downloading protobuf-3.20.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (698 bytes)\nDownloading protobuf-3.20.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: protobuf\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.19.6\n    Uninstalling protobuf-3.19.6:\n      Successfully uninstalled protobuf-3.19.6\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.0.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\ngoogle-ai-generativelanguage 0.6.10 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\ngoogle-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-artifact-registry 1.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-bigquery-connection 1.15.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-datastore 2.20.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-dlp 3.18.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-functions 1.16.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-iam 2.15.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-language 2.14.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-monitoring 2.21.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-pubsub 2.21.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-resource-manager 1.12.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-spanner 3.47.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-videointelligence 2.13.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\ngoogleapis-common-protos 1.63.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\ngrpc-google-iam-v1 0.12.7 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nonnx 1.17.0 requires protobuf>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\ntensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\ntensorflow-decision-forests 1.9.1 requires tensorflow~=2.16.1, but you have tensorflow 2.11.0 which is incompatible.\ntensorflow-serving-api 2.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\ntensorflow-serving-api 2.16.1 requires tensorflow<3,>=2.16.1, but you have tensorflow 2.11.0 which is incompatible.\ntensorflow-text 2.16.1 requires tensorflow<2.17,>=2.16.1; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.11.0 which is incompatible.\ntf-keras 2.16.0 requires tensorflow<2.17,>=2.16, but you have tensorflow 2.11.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-3.20.1\nCollecting faiss-cpu==1.9.0.post1\n  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\nDownloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.9.0.post1\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: easyocr in /opt/conda/lib/python3.10/site-packages (1.7.2)\nRequirement already satisfied: spacy in /opt/conda/lib/python3.10/site-packages (3.7.6)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from easyocr) (2.4.0)\nRequirement already satisfied: torchvision>=0.5 in /opt/conda/lib/python3.10/site-packages (from easyocr) (0.19.0)\nRequirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.10/site-packages (from easyocr) (4.10.0.84)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from easyocr) (1.14.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from easyocr) (10.3.0)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from easyocr) (0.23.2)\nRequirement already satisfied: python-bidi in /opt/conda/lib/python3.10/site-packages (from easyocr) (0.6.0)\nRequirement already satisfied: Shapely in /opt/conda/lib/python3.10/site-packages (from easyocr) (1.8.5.post1)\nRequirement already satisfied: pyclipper in /opt/conda/lib/python3.10/site-packages (from easyocr) (1.3.0.post5)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from easyocr) (1.11.1.1)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (8.2.5)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (0.12.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.9.2)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.1.4)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy) (70.0.0)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.4.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\nRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->easyocr) (3.3)\nRequirement already satisfied: imageio>=2.33 in /opt/conda/lib/python3.10/site-packages (from scikit-image->easyocr) (2.34.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->easyocr) (2024.5.22)\nRequirement already satisfied: lazy-loader>=0.4 in /opt/conda/lib/python3.10/site-packages (from scikit-image->easyocr) (0.4)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->easyocr) (1.13.3)\nRequirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->easyocr) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.44.1\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nCollecting sentence_transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nCollecting python-docx\n  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: openpyxl in /opt/conda/lib/python3.10/site-packages (3.1.5)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nRequirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.25.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (10.3.0)\nRequirement already satisfied: lxml>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from python-docx) (5.3.0)\nRequirement already satisfied: typing-extensions>=4.9.0 in /opt/conda/lib/python3.10/site-packages (from python-docx) (4.12.2)\nRequirement already satisfied: et-xmlfile in /opt/conda/lib/python3.10/site-packages (from openpyxl) (1.1.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m566.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: python-docx, sentence_transformers\nSuccessfully installed python-docx-1.1.2 sentence_transformers-3.3.1\nCollecting ru-core-news-sm==3.7.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.7.0/ru_core_news_sm-3.7.0-py3-none-any.whl (15.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from ru-core-news-sm==3.7.0) (3.7.6)\nCollecting pymorphy3>=1.0.0 (from ru-core-news-sm==3.7.0)\n  Downloading pymorphy3-2.0.2-py3-none-any.whl.metadata (1.8 kB)\nCollecting dawg-python>=0.7.1 (from pymorphy3>=1.0.0->ru-core-news-sm==3.7.0)\n  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\nCollecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-sm==3.7.0)\n  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (8.2.5)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.12.3)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (4.66.4)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.9.2)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.1.4)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (70.0.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.4.1)\nRequirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.26.4)\nRequirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.1.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.23.4)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2024.8.30)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.1.4)\nRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (13.7.1)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.19.0)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (7.0.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.1.5)\nRequirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.1.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.18.0)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.16.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.1.2)\nDownloading pymorphy3-2.0.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\nDownloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg-python, pymorphy3, ru-core-news-sm\nSuccessfully installed dawg-python-0.7.2 pymorphy3-2.0.2 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-sm-3.7.0\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('ru_core_news_sm')\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install python-docx\n!pip install PyMuPDF\n!pip install sentence-transformers ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T17:40:11.633561Z","iopub.execute_input":"2024-12-01T17:40:11.633930Z","iopub.status.idle":"2024-12-01T17:40:31.374321Z","shell.execute_reply.started":"2024-12-01T17:40:11.633894Z","shell.execute_reply":"2024-12-01T17:40:31.373375Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: python-docx in /opt/conda/lib/python3.10/site-packages (1.1.2)\nRequirement already satisfied: lxml>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from python-docx) (5.3.0)\nRequirement already satisfied: typing-extensions>=4.9.0 in /opt/conda/lib/python3.10/site-packages (from python-docx) (4.12.2)\nCollecting PyMuPDF\n  Downloading PyMuPDF-1.24.14-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\nDownloading PyMuPDF-1.24.14-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (19.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: PyMuPDF\nSuccessfully installed PyMuPDF-1.24.14\n\u001b[31mERROR: Invalid requirement: 'я'\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n\nhf_token = \"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\"\nmodel_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\nbnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n\n# Устанавливаем токенизатор\nllm_tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n\nif llm_tokenizer.pad_token_id is None:\n    llm_tokenizer.pad_token_id = llm_tokenizer.eos_token_id\n\n# Загружаем модель с автоматическим распределением по доступным GPU\nllm_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    token=hf_token,\n    quantization_config=bnb_config,\n    device_map=\"auto\",  # Автоматическое распределение по доступным GPU\n    torch_dtype=torch.float16,\n)\n\nprint(\"Модели загружены.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T17:41:08.794190Z","iopub.execute_input":"2024-12-01T17:41:08.794997Z","iopub.status.idle":"2024-12-01T17:47:36.951498Z","shell.execute_reply.started":"2024-12-01T17:41:08.794957Z","shell.execute_reply":"2024-12-01T17:47:36.950553Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/141k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7e32f51cdf947148736f7d703b5408b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47a937d6a3ff40fe9a5c8b36db10f539"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"436e2cc1996f429188230b44e0051c69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13813afc12d24df7b812e93d716b13f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"651b9f260ab84bc4a140b3560d409191"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fbb1433785c4416905d3619cabf492b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90d160c0747543aa8ac79d6073f31e7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a319c629bc694174b945ed490d421f22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a41aaebbf7b0422bb99d4a6a66b5d77d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de206d8562d14a67ad3a0d9d042260d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"276eaacb54814d84ad64fb291cf99636"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e8e362f1e924faf99676838fbcd0631"}},"metadata":{}},{"name":"stdout","text":"Модели загружены.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport os\nimport time\nfrom PIL import Image, ImageEnhance, ImageFilter\nimport easyocr\nimport cv2\nimport numpy as np\nimport torch  # Для использования GPU\n\n\n\n# Преобразование изображения в черно-белое и улучшение контраста\ndef preprocess_image(image_path):\n    \"\"\"\n    Преобразует изображение в черно-белое, увеличивает контраст, уменьшает шум, применяет адаптивный порог\n    и масштабирует изображение для улучшения разрешения.\n    \"\"\"\n    img = Image.open(image_path)\n    \n    # Масштабирование изображения для улучшения разрешения\n    width, height = img.size\n    new_width = int(width * 1.5)  # Увеличиваем размер изображения\n    new_height = int(height * 1.5)\n    img = img.resize((new_width, new_height), Image.LANCZOS)  # Используем Lanczos для увеличения\n    \n    # Преобразование изображения в черно-белое\n    img = img.convert('L')\n    \n    # Увлажнение контраста\n    enhancer = ImageEnhance.Contrast(img)\n    img = enhancer.enhance(2)  # Увеличиваем контраст\n    \n    # Удаление шума (например, через медианный фильтр)\n    img = img.filter(ImageFilter.MedianFilter(size=3))  # Медианный фильтр для уменьшения шума\n    \n    # Адаптивный порог для улучшения разделения текста и фона\n    img_cv = np.array(img)\n    img_cv = cv2.adaptiveThreshold(img_cv, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n    img = Image.fromarray(img_cv)\n    \n    # Дополнительное сглаживание\n    img = img.filter(ImageFilter.GaussianBlur(radius=1))\n    \n    return img\n\n# Пример использования OCR с улучшением изображения\ndef ocr_image(image_path):\n    \"\"\"\n    Извлекает текст из изображения с помощью EasyOCR на GPU.\n    \"\"\"\n    try:\n        # Явно указываем, какой GPU использовать (например, cuda:0 для первого GPU)\n        torch.cuda.set_device(0)  # Используем GPU 0, если у тебя несколько GPU\n        \n        # Предварительная обработка изображения\n        processed_image = preprocess_image(image_path)\n        processed_image_path = \"processed_image.png\"\n        processed_image.save(processed_image_path)\n        \n        # Инициализация OCR с GPU\n        reader = easyocr.Reader(['ru'], gpu=True)\n        result = reader.readtext(processed_image_path)\n        \n        # Извлекаем и выводим текст\n        text = \" \".join([item[1] for item in result])\n        \n        # Печатаем только первые 50 символов для проверки\n        print(f\"Обработан текст из изображения {image_path}: {text[:50]}...\")\n        \n        return text\n    except Exception as e:\n        print(f\"Ошибка при обработке изображения {image_path}: {e}\")\n        return \"\"\n    \n# Функция для извлечения текста из PDF\ndef extract_text_from_pdf(pdf_path):\n    import fitz  # PyMuPDF\n    doc = fitz.open(pdf_path)\n    text = \"\"\n    for page_num in range(len(doc)):\n        page = doc.load_page(page_num)\n        text += page.get_text()\n    return text\n\n# Функция для извлечения текста из DOCX\ndef extract_text_from_docx(docx_path):\n    from docx import Document\n    doc = Document(docx_path)\n    text = \"\"\n    for para in doc.paragraphs:\n        text += para.text\n    return text\n\n# Функция для очистки памяти GPU\ndef clear_gpu_memory():\n    \"\"\"\n    Освобождает память GPU после выполнения операций\n    \"\"\"\n    torch.cuda.empty_cache()\n\n# Основная функция для обработки файлов\n# Основная функция для обработки файлов\nfile_1 = '/kaggle/input/testoviy/test.docx'\nfile_2 = '/kaggle/input/testoviy/test.pdf'\nfile_3 = '/kaggle/input/testoviy/test.jpg'\nfile_4 = '/kaggle/input/testoviy/test1.jpg'\nfile_5 = '/kaggle/input/testoviy/test.txt'\n\n\nwith open(file_5, 'r', encoding='utf-8') as f:\n                file_text_txt = f.read()\nfile_text_ocr = ocr_image(file_3)\nfile_text_ocr_2 = ocr_image(file_4)\nfile_text_docx = extract_text_from_docx(file_1)\nfile_text_pdf = extract_text_from_pdf(file_2)\n\n\n                 \n# Промпты для разных типов файлов\nprompt_txt = f\"Ты работаешь с обычным текстовым файлом. Пожалуйста, разбей текст на логичные смысловые блоки, исходя из контекста и структуры документа. Ищите естественные переходы между абзацами, идеями и концепциями. Убедись, что каждый блок имеет свою завершенную мысль.\\nТекст: {file_text_txt}\",\nprompt_docx = f\"Ты анализируешь документ в формате .docx с несколькими абзацами и заголовками. Используя структуру документа (заголовки, подзаголовки), выдели ключевые разделы и параграфы. Сохрани последовательность мысли, разделяя текст на логичные блоки, избегая чрезмерных разрывов.\\nТекст: {file_text_docx}\",\nprompt_pdf= f\"Это текст из PDF файла, который может содержать ненужные символы и шум (например, номера страниц, случайные символы). Проанализируй текст, игнорируя все несущественное, и раздели его на логичные смысловые блоки, основываясь на контексте, а также на изменениях в стиле текста (например, заголовки, выделения).\\nТекст: {file_text_pdf}\",\nprompt_ocr = f\"Это текст, извлеченный с изображения с помощью easyocr.Проанализируй его, исправь орфографические ошибки, устрань искажения и шумы, которые могут возникнуть при распознавании (например, замену букв, чисел, символов, таких как 'и' на 'л', 'О' на '0' и т.д.). Структурируй текст, разделяя его на логичные блоки, восстанавливай пропущенные элементы и корректно оформляй предложения. Учитывай контекст, чтобы восстановить точное значение и сделать текст более читаемым и правильным. Проверь текст на смысловые ошибки и искажения, связанные с неправильным распознаванием символов и слов. После приведения текста в порядок, выполните логичный семантический анализ. Разделите текст на смысловые блоки, исходя из контекста и структуры документа. Ищите естественные переходы между абзацами, идеями и концепциями. Убедитесь, что каждый блок имеет завершенную мысль и логичное завершение.\\nТекст: {file_text_ocr}\"\nprompt_ocr_2 = f\"Это текст, извлеченный с изображения с помощью easyocr.Проанализируй его, исправь орфографические ошибки, устрань искажения и шумы, которые могут возникнуть при распознавании (например, замену букв, чисел, символов, таких как 'и' на 'л', 'О' на '0' и т.д.). Структурируй текст, разделяя его на логичные блоки, восстанавливай пропущенные элементы и корректно оформляй предложения. Учитывай контекст, чтобы восстановить точное значение и сделать текст более читаемым и правильным. Проверь текст на смысловые ошибки и искажения, связанные с неправильным распознаванием символов и слов. После приведения текста в порядок, выполните логичный семантический анализ. Разделите текст на смысловые блоки, исходя из контекста и структуры документа. Ищите естественные переходы между абзацами, идеями и концепциями. Убедитесь, что каждый блок имеет завершенную мысль и логичное завершение.\\nТекст: {file_text_ocr_2}\"\n\n\n\n# Освобождаем память перед генерацией\ntorch.cuda.empty_cache()\n\n# Перемещаем входные данные на тот же GPU, что и модель\ninput_ids_txt = llm_tokenizer(prompt_txt, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n\n\n# Генерация результата с ограничением max_new_tokens для экономии памяти\nwith torch.no_grad():\n            output = llm_model.generate(\n                input_ids=input_ids_txt,\n                max_new_tokens=512,  # Ограничение по количеству новых токенов\n                num_return_sequences=1,\n                temperature=0.7,\n            )\n        \n        # Выводим результат\nprint(f\"Обработан файл {file_5}: {llm_tokenizer.decode(output[0], skip_special_tokens=True)}\")\n\n        # Очистка памяти после обработки\nclear_gpu_memory()\n\n\ninput_ids_docx = llm_tokenizer(prompt_docx, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n\n\n# Генерация результата с ограничением max_new_tokens для экономии памяти\nwith torch.no_grad():\n            output = llm_model.generate(\n                input_ids=input_ids_docx,\n                max_new_tokens=512,  # Ограничение по количеству новых токенов\n                num_return_sequences=1,\n                temperature=0.7,\n            )\n        \n        # Выводим результат\nprint(f\"Обработан файл {file_1}: {llm_tokenizer.decode(output[0], skip_special_tokens=True)}\")\n\n        # Очистка памяти после обработки\nclear_gpu_memory()\n\n\ninput_ids_pdf = llm_tokenizer(prompt_pdf, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n\n\n# Генерация результата с ограничением max_new_tokens для экономии памяти\nwith torch.no_grad():\n            output = llm_model.generate(\n                input_ids=input_ids_pdf,\n                max_new_tokens=512,  # Ограничение по количеству новых токенов\n                num_return_sequences=1,\n                temperature=0.7,\n            )\n        \n        # Выводим результат\nprint(f\"Обработан файл {file_2}: {llm_tokenizer.decode(output[0], skip_special_tokens=True)}\")\n\n        # Очистка памяти после обработки\nclear_gpu_memory()\n\n\n\ninput_ids_ocr = llm_tokenizer(prompt_ocr, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n# Генерация результата с ограничением max_new_tokens для экономии памяти\nwith torch.no_grad():\n            output = llm_model.generate(\n                input_ids=input_ids_ocr,\n                max_new_tokens=512,  # Ограничение по количеству новых токенов\n                num_return_sequences=1,\n                temperature=0.7,\n            )\n        \n        # Выводим результат\nprint(f\"Обработан файл {file_3}: {llm_tokenizer.decode(output[0], skip_special_tokens=True)}\")\n\n        # Очистка памяти после обработки\nclear_gpu_memory()\n\n\ninput_ids_ocr_2 = llm_tokenizer(prompt_ocr_2, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n# Генерация результата с ограничением max_new_tokens для экономии памяти\nwith torch.no_grad():\n            output = llm_model.generate(\n                input_ids=input_ids_ocr_2,\n                max_new_tokens=512,  # Ограничение по количеству новых токенов\n                num_return_sequences=1,\n                temperature=0.7,\n            )\n        \n        # Выводим результат\nprint(f\"Обработан файл {file_4}: {llm_tokenizer.decode(output[0], skip_special_tokens=True)}\")\n\n        # Очистка памяти после обработки\nclear_gpu_memory()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T18:38:03.248517Z","iopub.execute_input":"2024-12-01T18:38:03.248903Z","iopub.status.idle":"2024-12-01T18:41:13.552834Z","shell.execute_reply.started":"2024-12-01T18:38:03.248864Z","shell.execute_reply":"2024-12-01T18:41:13.551883Z"}},"outputs":[{"name":"stdout","text":"Обработан текст из изображения /kaggle/input/testoviy/test.jpg: 6} последние Голы искусственный интеллект (иИ стап...\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Обработан текст из изображения /kaggle/input/testoviy/test1.jpg: ОШШИЬКА; [реобразовачель зависимосшей [р в насш@ящ...\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Обработан файл /kaggle/input/testoviy/test.txt: Ты работаешь с обычным текстовым файлом. Пожалуйста, разбей текст на логичные смысловые блоки, исходя из контекста и структуры документа. Ищите естественные переходы между абзацами, идеями и концепциями. Убедись, что каждый блок имеет свою завершенную мысль.\nТекст: В последние годы искусственный интеллект (ИИ) стал неотъемлемой частью многих отраслей, оказывая значительное влияние на различные аспекты жизни и работы людей. Одним из самых перспективных направлений является использование ИИ для обработки и анализа текстов. Семантический анализ текста, как важнейшая часть обработки естественного языка (NLP), позволяет машинам «понимать» текст, извлекая из него не только слова, но и их смысл. В отличие от традиционных методов обработки текста, которые фокусируются на синтаксическом анализе, семантический подход требует глубокого осмысления, выявления взаимосвязей между словами и их контекстом.\nСемантический анализ текста включает в себя несколько ключевых этапов. На первом этапе происходит разбиение текста на элементы, такие как слова и фразы, а также идентификация их значений. Затем система пытается установить, как эти элементы связаны между собой, что помогает определить общий смысл текста. Этот процесс включает использование различных алгоритмов и моделей машинного обучения, которые могут учитывать контекст, многозначность слов, а также особенности языка, такие как идиомы или метафоры.\nДля успешного семантического анализа текстов требуется наличие больших объемов данных, а также мощных вычислительных ресурсов. Современные модели, такие как трансформеры и нейронные сети, значительно повысили точность и эффективность таких анализов. Одним из наиболее известных примеров таких моделей является GPT (Generative Pre-trained Transformer), которая использует огромные объемы текстовых данных для обучения и позволяет эффективно решать задачи, связанные с анализом и генерацией текста. Однако для успешного применения таких моделей важно, чтобы они были настроены на решение конкретных задач и учитывали особенности предметной области.\nОдной из задач, решаемых с помощью семантического анализа, является извлечение информации из больших объемов текста. Это может быть полезно, например, в области автоматической обработки документов, анализа мнений в социальных сетях, а также для создания систем рекомендаций. Например, с помощью анализа текстов можно выделять ключевые темы и идеи, которые выражаются в тексте, что позволяет классифицировать информацию, а также находить ответы на вопросы, заданные пользователем. В частности, семантический анализ активно используется в чат-ботах и виртуальных помощниках, где точное понимание запросов пользователей имеет решающее значение для корректной работы системы.\nКроме того, семантический анализ текста играет важную роль в сфере маркетинга и рекламы. С помощью ИИ можно анализировать отзывы клиентов, сообщения в социальных сетях, статьи и другие источники информации, чтобы выявить настроения, предпочтения и потребности аудитории. Это позволяет компаниям более точно настраивать свои рекламные кампании, а также адаптировать продукцию под запросы потребителей. Семантический анализ помогает не только в определении положительных и отрицательных эмоций в тексте, но и в глубоком понимании намерений и мотивации людей, что является важным инструментом в принятии бизнес-решений.\nТакже стоит отметить роль семантического анализа в области безопасности и предотвращения угроз. Он может быть использован для анализа сообщений, писем или других текстов с целью обнаружения признаков мошенничества, спама или других форм злоупотреблений. Для этого анализируется не только форма и структура текста, но и его содержание, что позволяет выявлять скрытые намерения или небезопасные действия. Например, в случае с электронной почтой семантический анализ помогает выявить спуффинг (маскировку под доверенный источник), фишинг или другие формы мошенничества, связанные с попытками получить личные данные.\nСемантический анализ также играет важную роль в области здравоохранения. Врачи и исследователи используют текстовую информацию из медицинских исследований, отчетов, медицинских записей и других источников для выявления закономерностей и тенденций, что помогает в диагностике и лечении заболеваний. Например, система, основанная на семантическом анализе, может обработать большие массивы медицинских текстов, чтобы выявить новые методы лечения или прогнозирования заболеваний. В этой области важно, чтобы системы ИИ имели не только высокую точность в анализе текста, но и способность учитывать медицинский контекст.\nВ сфере образования семантический анализ используется для создания автоматических систем оценки и помощи студентам. Такие системы могут анализировать тексты эссе, предоставлять рекомендации по улучшению письма, а также выявлять потенциальные плагиаты. Также семантический анализ помогает в создании систем интеллектуальной поддержки, которые могут отвечать на вопросы студентов, используя имеющиеся данные. Это значительно улучшает образовательный процесс и способствует более глубокому пониманию материала.\nНе стоит забывать, что семантический анализ текста может быть полезен и в других областях, таких как юридическая практика, автоматизация перевода, создание интеллектуальных систем для обработки правовых документов и даже в искусственном искусстве. В каждом из этих случаев важна способность системы понимать контекст и нюансы текста, а также точно интерпретировать его значение, что позволяет улучшать качество обслуживания и принятие решений.\nВ целом, семантический анализ текста является важным инструментом, который значительно расширяет возможности ИИ и помогает решать множество задач в различных областях. С развитием технологий и алгоритмов можно ожидать, что в будущем системы станут еще более точными и эффективными, что откроет новые горизонты для их применения в бизнесе, науке и повседневной жизни.\n\n1. Искусственный интеллект (ИИ) стал неотъемлемой частью многих отраслей, оказывая значительное влияние на различные аспекты жизни и работы людей.\n2. Одним из самых перспективных направлений является использование ИИ для обработки и анализа текстов.\n3. Семантический анализ текста, как важнейшая часть обработки естественного языка (NLP), позволяет машинам «понимать» текст, извлекая из него не только слова, но и их смысл.\n4. В отличие от традиционных методов обработки текста, которые фокусируются на синтаксическом анализе, семантический подход требует глубокого осмысления, выявления взаимосвязей между словами и их контекстом.\n5. Для успешного семантического анализа текстов требуется наличие больших объемов данных, а также мощных вычислительных ресурсов.\n6. Современные модели, такие как трансформеры и нейронные сети, значительно повысили точность и эффективность таких анализов.\n7. Одной из задач, решаемых с помощью семантического анализа, является извлечение информации из больших объемов текста.\n8. Семантический анализ играет важную роль в сфере маркетинга и рекламы.\n9. Семантический анализ также играет важную роль в области безопасности и предотвращения угроз.\n10. Семантический анализ также играет важную роль в области здравоохранения.\n11. В сфере образования семантический анализ используется для создания автоматических систем о\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Обработан файл /kaggle/input/testoviy/test.docx: Ты анализируешь документ в формате .docx с несколькими абзацами и заголовками. Используя структуру документа (заголовки, подзаголовки), выдели ключевые разделы и параграфы. Сохрани последовательность мысли, разделяя текст на логичные блоки, избегая чрезмерных разрывов.\nТекст: В последние годы искусственный интеллект (ИИ) стал неотъемлемой частью многих отраслей, оказывая значительное влияние на различные аспекты жизни и работы людей. Одним из самых перспективных направлений является использование ИИ для обработки и анализа текстов. Семантический анализ текста, как важнейшая часть обработки естественного языка (NLP), позволяет машинам «понимать» текст, извлекая из него не только слова, но и их смысл. В отличие от традиционных методов обработки текста, которые фокусируются на синтаксическом анализе, семантический подход требует глубокого осмысления, выявления взаимосвязей между словами и их контекстом.Семантический анализ текста включает в себя несколько ключевых этапов. На первом этапе происходит разбиение текста на элементы, такие как слова и фразы, а также идентификация их значений. Затем система пытается установить, как эти элементы связаны между собой, что помогает определить общий смысл текста. Этот процесс включает использование различных алгоритмов и моделей машинного обучения, которые могут учитывать контекст, многозначность слов, а также особенности языка, такие как идиомы или метафоры.Для успешного семантического анализа текстов требуется наличие больших объемов данных, а также мощных вычислительных ресурсов. Современные модели, такие как трансформеры и нейронные сети, значительно повысили точность и эффективность таких анализов. Одним из наиболее известных примеров таких моделей является GPT (Generative Pre-trained Transformer), которая использует огромные объемы текстовых данных для обучения и позволяет эффективно решать задачи, связанные с анализом и генерацией текста. Однако для успешного применения таких моделей важно, чтобы они были настроены на решение конкретных задач и учитывали особенности предметной области.Одной из задач, решаемых с помощью семантического анализа, является извлечение информации из больших объемов текста. Это может быть полезно, например, в области автоматической обработки документов, анализа мнений в социальных сетях, а также для создания систем рекомендаций. Например, с помощью анализа текстов можно выделять ключевые темы и идеи, которые выражаются в тексте, что позволяет классифицировать информацию, а также находить ответы на вопросы, заданные пользователем. В частности, семантический анализ активно используется в чат-ботах и виртуальных помощниках, где точное понимание запросов пользователей имеет решающее значение для корректной работы системы.Кроме того, семантический анализ текста играет важную роль в сфере маркетинга и рекламы. С помощью ИИ можно анализировать отзывы клиентов, сообщения в социальных сетях, статьи и другие источники информации, чтобы выявить настроения, предпочтения и потребности аудитории. Это позволяет компаниям более точно настраивать свои рекламные кампании, а также адаптировать продукцию под запросы потребителей. Семантический анализ помогает не только в определении положительных и отрицательных эмоций в тексте, но и в глубоком понимании намерений и мотивации людей, что является важным инструментом в принятии бизнес-решений.Также стоит отметить роль семантического анализа в области безопасности и предотвращения угроз. Он может быть использован для анализа сообщений, писем или других текстов с целью обнаружения признаков мошенничества, спама или других форм злоупотреблений. Для этого анализируется не только форма и структура текста, но и его содержание, что позволяет выявлять скрытые намерения или небезопасные действия. Например, в случае с электронной почтой семантический анализ помогает выявить спуффинг (маскировку под доверенный источник), фишинг или другие формы мошенничества, связанные с попытками получить личные данные.Семантический анализ также играет важную роль в области здравоохранения. Врачи и исследователи используют текстовую информацию из медицинских исследований, отчетов, медицинских записей и других источников для выявления закономерностей и тенденций, что помогает в диагностике и лечении заболеваний. Например, система, основанная на семантическом анализе, может обработать большие массивы медицинских текстов, чтобы выявить новые методы лечения или прогнозирования заболеваний. В этой области важно, чтобы системы ИИ имели не только высокую точность в анализе текста, но и способность учитывать медицинский контекст.В сфере образования семантический анализ используется для создания автоматических систем оценки и помощи студентам. Такие системы могут анализировать тексты эссе, предоставлять рекомендации по улучшению письма, а также выявлять потенциальные плагиаты. Также семантический анализ помогает в создании систем интеллектуальной поддержки, которые могут отвечать на вопросы студентов, используя имеющиеся данные. Это значительно улучшает образовательный процесс и способствует более глубокому пониманию материала.Не стоит забывать, что семантический анализ текста может быть полезен и в других областях, таких как юридическая практика, автоматизация перевода, создание интеллектуальных систем для обработки правовых документов и даже в искусственном искусстве. В каждом из этих случаев важна способность системы понимать контекст и нюансы текста, а также точно интерпретировать его значение, что позволяет улучшать качество обслуживания и принятие решений.В целом, семантический анализ текста является важным инструментом, который значительно расширяет возможности ИИ и помогает решать множество задач в различных областях. С развитием технологий и алгоритмов можно ожидать, что в будущем системы станут еще более точными и эффективными, что откроет новые горизонты для их применения в бизнесе, науке и повседневной жизни.\n\n1. Искусственный интеллект (ИИ) и семантический анализ текста\n   - ИИ в различных отраслях\n   - Семантический анализ текста как часть обработки естественного языка (NLP)\n   - Отличия семантического анализа от традиционных методов обработки текста\n\n2. Этапы семантического анализа текста\n   - Разбиение текста на элементы и идентификация их значений\n   - Установление взаимосвязей между элементами и определение общий смысл текста\n   - Использование алгоритмов и моделей машинного обучения\n\n3. Требования к успешному семантическому анализу текстов\n   - Наличие больших объемов данных и мощных вычислительных ресурсов\n   - Современные модели, такие как трансформеры и нейронные сети\n   - Настройка моделей на решение конкретных задач и учитывание особенностей предметной области\n\n4. Применение семантического анализа текстов\n   - Извлечение информации из больших объемов текста\n   - Классификация информации и поиск ответов на вопросы\n   - Использование в чат-ботах и виртуальных помощниках\n   - Анализ отзывов клиентов, сообщений в социальных сетях, статей и других источников информации\n   - Определение положительных и отрицательных эмоций в тексте и глубокое понимание намерений и мотивации людей\n\n5. Роль семантического анализа в области безопасности и предотвращения угроз\n   - Обнаружение признаков мошенничества\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Обработан файл /kaggle/input/testoviy/test.pdf: Это текст из PDF файла, который может содержать ненужные символы и шум (например, номера страниц, случайные символы). Проанализируй текст, игнорируя все несущественное, и раздели его на логичные смысловые блоки, основываясь на контексте, а также на изменениях в стиле текста (например, заголовки, выделения).\nТекст: В последние годы искусственный интеллект (ИИ) стал неотъемлемой частью многих \nотраслей, оказывая значительное влияние на различные аспекты жизни и работы людей. \nОдним из самых перспективных направлений является использование ИИ для обработки и \nанализа текстов. Семантический анализ текста, как важнейшая часть обработки \nестественного языка (NLP), позволяет машинам «понимать» текст, извлекая из него не \nтолько слова, но и их смысл. В отличие от традиционных методов обработки текста, \nкоторые фокусируются на синтаксическом анализе, семантический подход требует \nглубокого осмысления, выявления взаимосвязей между словами и их контекстом. \nСемантический анализ текста включает в себя несколько ключевых этапов. На первом \nэтапе происходит разбиение текста на элементы, такие как слова и фразы, а также \nидентификация их значений. Затем система пытается установить, как эти элементы \nсвязаны между собой, что помогает определить общий смысл текста. Этот процесс \nвключает использование различных алгоритмов и моделей машинного обучения, которые \nмогут учитывать контекст, многозначность слов, а также особенности языка, такие как \nидиомы или метафоры. \nДля успешного семантического анализа текстов требуется наличие больших объемов \nданных, а также мощных вычислительных ресурсов. Современные модели, такие как \nтрансформеры и нейронные сети, значительно повысили точность и эффективность таких \nанализов. Одним из наиболее известных примеров таких моделей является GPT \n(Generative Pre-trained Transformer), которая использует огромные объемы текстовых \nданных для обучения и позволяет эффективно решать задачи, связанные с анализом и \nгенерацией текста. Однако для успешного применения таких моделей важно, чтобы они \nбыли настроены на решение конкретных задач и учитывали особенности предметной \nобласти. \nОдной из задач, решаемых с помощью семантического анализа, является извлечение \nинформации из больших объемов текста. Это может быть полезно, например, в области \nавтоматической обработки документов, анализа мнений в социальных сетях, а также для \nсоздания систем рекомендаций. Например, с помощью анализа текстов можно выделять \nключевые темы и идеи, которые выражаются в тексте, что позволяет классифицировать \nинформацию, а также находить ответы на вопросы, заданные пользователем. В частности, \nсемантический анализ активно используется в чат-ботах и виртуальных помощниках, где \nточное понимание запросов пользователей имеет решающее значение для корректной \nработы системы. \nКроме того, семантический анализ текста играет важную роль в сфере маркетинга и \nрекламы. С помощью ИИ можно анализировать отзывы клиентов, сообщения в \nсоциальных сетях, статьи и другие источники информации, чтобы выявить настроения, \nпредпочтения и потребности аудитории. Это позволяет компаниям более точно \nнастраивать свои рекламные кампании, а также адаптировать продукцию под запросы \nпотребителей. Семантический анализ помогает не только в определении положительных и \nотрицательных эмоций в тексте, но и в глубоком понимании намерений и мотивации \nлюдей, что является важным инструментом в принятии бизнес-решений. \nТакже стоит отметить роль семантического анализа в области безопасности и \nпредотвращения угроз. Он может быть использован для анализа сообщений, писем или \nдругих текстов с целью обнаружения признаков мошенничества, спама или других форм \nзлоупотреблений. Для этого анализируется не только форма и структура текста, но и его \nсодержание, что позволяет выявлять скрытые намерения или небезопасные действия. \nНапример, в случае с электронной почтой семантический анализ помогает выявить \nспуффинг (маскировку под доверенный источник), фишинг или другие формы \nмошенничества, связанные с попытками получить личные данные. \nСемантический анализ также играет важную роль в области здравоохранения. Врачи и \nисследователи используют текстовую информацию из медицинских исследований, \nотчетов, медицинских записей и других источников для выявления закономерностей и \nтенденций, что помогает в диагностике и лечении заболеваний. Например, система, \nоснованная на семантическом анализе, может обработать большие массивы медицинских \nтекстов, чтобы выявить новые методы лечения или прогнозирования заболеваний. В этой \nобласти важно, чтобы системы ИИ имели не только высокую точность в анализе текста, \nно и способность учитывать медицинский контекст. \nВ сфере образования семантический анализ используется для создания автоматических \nсистем оценки и помощи студентам. Такие системы могут анализировать тексты эссе, \nпредоставлять рекомендации по улучшению письма, а также выявлять потенциальные \nплагиаты. Также семантический анализ помогает в создании систем интеллектуальной \nподдержки, которые могут отвечать на вопросы студентов, используя имеющиеся данные. \nЭто значительно улучшает образовательный процесс и способствует более глубокому \nпониманию материала. \nНе стоит забывать, что семантический анализ текста может быть полезен и в других \nобластях, таких как юридическая практика, автоматизация перевода, создание \nинтеллектуальных систем для обработки правовых документов и даже в искусственном \nискусстве. В каждом из этих случаев важна способность системы понимать контекст и \nнюансы текста, а также точно интерпретировать его значение, что позволяет улучшать \nкачество обслуживания и принятие решений. \nВ целом, семантический анализ текста является важным инструментом, который \nзначительно расширяет возможности ИИ и помогает решать множество задач в различных \nобластях. С развитием технологий и алгоритмов можно ожидать, что в будущем системы \nстанут еще более точными и эффективными, что откроет новые горизонты для их \nприменения в бизнесе, науке и повседневной жизни. \n \n1. Искусственный интеллект (ИИ) стал неотъемлемой частью многих отраслей, оказывая значительное влияние на различные аспекты жизни и работы людей.\n2. Одним из самых перспективных направлений является использование ИИ для обработки и анализа текстов.\n3. Семантический анализ текста, как важнейшая часть обработки естественного языка (NLP), позволяет машинам «понимать» текст, извлекая из него не только слова, но и их смысл.\n4. Семантический анализ текста включает в себя несколько ключевых этапов: разбиение текста на элементы, идентификация их значений, установление взаимосвязей между элементами и определение общий смысл текста.\n5. Для успешного семантического анализа текстов требуется наличие больших объемов данных, а также мощных вычислительных ресурсов.\n6. Семантический анализ текста играет важную роль в сфере маркетинга и рекламы, в области безопасности и предотвращения угроз, в области здравоохранения, в сфере образования и в других областях.\n7. В будущем системы семантического анализа текста могут стать еще более точными и эффективными, что откроет новые горизонты для их применения в бизнесе, науке и повседневной жизни.\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Обработан файл /kaggle/input/testoviy/test.jpg: Это текст, извлеченный с изображения с помощью easyocr.Проанализируй его, исправь орфографические ошибки, устрань искажения и шумы, которые могут возникнуть при распознавании (например, замену букв, чисел, символов, таких как 'и' на 'л', 'О' на '0' и т.д.). Структурируй текст, разделяя его на логичные блоки, восстанавливай пропущенные элементы и корректно оформляй предложения. Учитывай контекст, чтобы восстановить точное значение и сделать текст более читаемым и правильным. Проверь текст на смысловые ошибки и искажения, связанные с неправильным распознаванием символов и слов. После приведения текста в порядок, выполните логичный семантический анализ. Разделите текст на смысловые блоки, исходя из контекста и структуры документа. Ищите естественные переходы между абзацами, идеями и концепциями. Убедитесь, что каждый блок имеет завершенную мысль и логичное завершение.\nТекст: 6} последние Голы искусственный интеллект (иИ стап неотъемлемой частью (ногих @отраслей; оказывая вначительное @лияние на различные аспекты &изни 0 работы пюдей Одним из самых перспективных направлений является использование &и для ббработки 0 @нализа текстовь Сёмантический анализ текстаь как вахнейшая часть обработки @стественного языка @1э)э позволяет Машинам @понимать2 тексто извлекая &з него не &олько Словао но 0 &х Смысль 6 @тличие @7 традиционных &етодов обработки &екстаэ ёоторые @окусируются на синтаксическом анализе семантический бодход требует (лубокого осмысленияь (ыявления езаимосвязей мехду Словами 0 их (онтекстомь Сёмантический анализ текста включает 6 себя несколько (лючевых отаповь (а (ервом отапе @роисходит разбиение &екста на_@лементы такие как Слова 0 @разы а гаке идентификашия их вначенийЬ Затем Система Пытается установитьо &ак ет0 олементы связаны мехду Собой} Что помогает Опрелелиты обший Смысл} текстаь Эгот процесс (ключает использование различных @лгоритмов 0 Ооделей; машинного @бучения которые Могуч учитывать [онтекст &ногозначность Слов} @ пакхе @собённости языка; Факие &ак идиомы Оли метафорыь {ля успешного семантического анализа текстов {ребуется Наличие больших обьемов Данныхэ а такхе Сошных (ычислительных ресурсовь Современные подели} такие как трансформеры 0 нейронные сети вначительно (овысили {очность 0 эффективносты таких анализовь Одним из наиболее известных @римеров паких поделей являётся @Рт (&@@гас#  @ге:#га#еЧ {газ огшег); Которая использует @промные @бьемы текстовых &анных &<Я @бучения 0 позволяет оФфективно решать вадачио связанные & анализом 0 (енерацией пекстаь Однако &ля  успешного применения таких (оделей вахноь &тобы они были (астроены на решение (онкретных вадач 0 учитывалй особенности Фелметной области Одной ив вадач решаемых & помощью семантического аналиваь является извлечение информации из больших обьемов текстаь Это похет быть полезно5 напримерь @области автоматической обработки документовь аналива Онений @ Социальных сетяхо @ такхе &ля Создания систем рекомендацийь (апримеро & помощью анализа пекстов Мохно (ылелять ключевые гемы 0 идеиь Которые (ырахаются 6 &екстеь ято позволяе? (лассифицировать информациюь @ такке находить Ответы на вопросы} ваданные пользователемы 6 частности: Семантический анализ} активно используется @ Чат-ботах 0 виртуальных: Оомощникахо СДе точное понимание вапросов (ользователей Имеет рёшающее вначение Для корректной работы системь Кроме погоь Семантический анализ) текста Играет вахную роль 6 сфере баркетинга 0 рекламыь & помощью ИИ Гохно анализировать @тзывы (лиентовь сообшения @ Социальных сетяхь Статьи 0 другие источники йнформациио чтобы выявить настроенияь @релпочтения 0 потребности @удитории Это (озволяет Компаниям более_почно настраивать свои рекламные кампаниио а также @даптировать_продукцию под вапросы потребителейь: Семантический) анализ помогает не полько @ определении Полохительных 0 отрицательных 6моций @ текстеь но 0 8 (лубоком понимании (амерений 0 Мотивации &юдейь &1о является вахным Инструментом 6 @ринятии бизнес-решенийь Такке Стои7 отметиты боль семантического анализа ₽ области безопасности 0 @редотвращения упрозь Он Сохет быь использован Для @нализа Сообщений} писем или других &екстов & шелью обнарухения ризнаков (ошенничества Спама или @ругих @орм влоупотреблений &ля этого анализируется не {олько @орма 0 структура текстаь но 0 @го Содерханиео &то позволяет (ыявлять Скрытые намерения или небезопасные действияь Напримерь 6 Случае & Блектронной почтой Семантический  анализ Помогает выявичы СпуфФинс (маскировку под Доверенный источник)о Фишинг или другие @ормы Мошенничестваа связанные & попытками получить личные данныеь Семантический @нализ} тахе Играет @ахную роль @ области вдравоохраненияь  Врачи 0 исслелователи ОСпользуюп текстовую инФормацию Из мелицинских исслелований} отчетов; Оедицинских ваписей 0 Других Иисточников} пля (ыявления вакономерностей 0 тенденций что помогает @ диагностике 0 лечении ваболеванийь_Напримеро Системаэ: основанная на семантическом @нализеь Гохет обработать большие &ассивы Мелицинских текстовь чтобы выявить (овые методы] лечения Или @рогнозирования ваболеваний}  8 этой области Вахноь чгобы} системы ( имели не {олько высокую точность в анализе &екста; но 0 Способность учитывать Седицинский контексто 6 сфере ббразования семантический анализ} используется &ля создания автоматических систем @енки 0 понощи Студентань   Такие системы Гогуу анализировать тексты эссеь предоставлять рекомендации (0 улучшению письмаь @ такке выявлять потенциальные @лагиатыь   Такхе семантический анализ} помогает @ Созданий) Систем Интеллектуальной) поллержкио которыег могут отвечать на @опросы студентовь используя имеющиеся данныеь_Это вначительно Улучшает образовательный процесс 0 способствует более Слубокому пониманию материалаь Не стоит вабыватьа &т0 Семантический ёнализ текста Оохёт быть @олёзен 0 @ Других областяхо таких &ак (ридическая практикаь автоматизация перевода; создание Интеллектуальных систем &ля обработки правовых @окументов 0 дахе 0 искусственном искусствеь 6 (ахлом из этих Случаев вахна Способность Системы понимать контекс7 0 Нюансы &екстаь @ такхе почно интерпретировать @го вначение; 91о позволяет Улучшаты качествообслуживания 0 Принятие решений: 6 (еломь семантический @нализ текста является Вахным инсярументомь который вначительно расширяе? возмохности ии 0 помогает решать Мнохество вадач @ различных @бластяхь & развитием технологий & @лгоритмов Мохно охидатьо ато 6 будущем системы станут еще болеё &очными 0 эффективными} что откроет новыё Соризонты пля их применения 6 бизнесеь  науке 0 повседневной &изниэ жизни.\n\nИскусственный интеллект (ИИ) является неотъемлемой частью многих отраслей, особенно в области обработки текстов. Сёмантический анализ текста является важнейшей частью обработки естественного языка. Машины понимают текст, извлекая не только слова, но и смысл.\n\nСемантический анализ текста включает в себя несколько этапов: разбиение текста на элементы, такие как слова, разы и гаке, идентификация этих элементов, а затем попытка установить, как эти элементы связаны между собой. Это позволяет определить общий смысл текста.\n\nДля успешного семантического анализа текстов требуется наличие больших объемов данных и совместных (вычислительных) ресурсов. Современные подели, такие как трансформеры и нейронные сети, в настоящее время являются очень эффективными в этом анализе.\n\nОдним из наиболее известных примеров таких поделей является Rt (гас#  @ге:#га#еЧ), которая использует простые алгоритмы текстовых анализов для решения различных задач, связанных с анализом текстов.\n\nСемантический анализ текста активно используется в области автоматической обработки документов, в социальных сетях, анализа рекламы, в области взаимодействия с клиентами, в области безопасности, в области медицины и в области образования.\n\nВ области безопасности семантический анализ используется для анализа сообщений, писем или других источников информации, чтобы выявить\nОбработан файл /kaggle/input/testoviy/test1.jpg: Это текст, извлеченный с изображения с помощью easyocr.Проанализируй его, исправь орфографические ошибки, устрань искажения и шумы, которые могут возникнуть при распознавании (например, замену букв, чисел, символов, таких как 'и' на 'л', 'О' на '0' и т.д.). Структурируй текст, разделяя его на логичные блоки, восстанавливай пропущенные элементы и корректно оформляй предложения. Учитывай контекст, чтобы восстановить точное значение и сделать текст более читаемым и правильным. Проверь текст на смысловые ошибки и искажения, связанные с неправильным распознаванием символов и слов. После приведения текста в порядок, выполните логичный семантический анализ. Разделите текст на смысловые блоки, исходя из контекста и структуры документа. Ищите естественные переходы между абзацами, идеями и концепциями. Убедитесь, что каждый блок имеет завершенную мысль и логичное завершение.\nТекст: ОШШИЬКА; [реобразовачель зависимосшей [р в насш@ящее время не учиываег все Усчан@вленные Шакешы Такое @оведениеявляешся ИСФ@ЧИКом следующих. [онфлИкаов зависимосшейь [р в насш@ящее время не учиываег все Усчан@вленные Шакешы Такое @оведениеявляешся ИСФ@ЧИКом следующих. [онфлИкаов зависимосшейь [р в насш@ящее время не учиываег все Усчан@вленные Шакешы Такое @оведениеявляешся ИСФ@ЧИКом следующих. [онфлИкаов зависимосшейь [р в насш@ящее время не учиываег все Усчан@вленные Шакешы Такое @оведениеявляешся ИСФ@ЧИКом следующих.\n\nПеревод: Образовательная программа зависимостей: в настоящее время не учитывается всё Усчанвленное Шакешье, такое образование является ИСФИЧИКом следующих: онфлИкаов зависимостей, в настоящее время не учитывается всё Усчанвленное Шакешье, такое образование является ИСФИЧИКом следующих: онфлИкаов зависимостей, в настоящее время не учитывается всё Усчанвленное Шакешье, такое образование является ИСФИЧИКом следующих: онфлИкаов зависимостей.\n\nПравильный текст: Образовательная программа зависимостей (ОПЗ) в настоящее время не учитывается всё Усчанвленное Шакешье. Такое образование является ИСФИЧИКом следующих: онфлИкаов зависимостей, в настоящее время не учитывается всё Усчанвленное Шакешье, такое образование является ИСФИЧИКом следующих: онфл\n","output_type":"stream"}],"execution_count":22}]}